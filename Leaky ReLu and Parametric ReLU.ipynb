{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e78695e-551e-43ab-93ab-ae010712d352",
   "metadata": {},
   "source": [
    "# Leaky ReLU এবং Parametric ReLU (PReLU) অ্যাক্টিভেশন ফাংশন\n",
    "\n",
    "**Leaky ReLU** এবং **Parametric ReLU (PReLU)** হলো ReLU ফাংশনের উন্নত সংস্করণ, যা **\"Dying ReLU\"** সমস্যার সমাধান দিতে তৈরি করা হয়েছে। এদের মূল পার্থক্য হলো, যখন ইনপুট নেগেটিভ হয়, তখনও এটি ছোট একটি স্লোপ রাখে।\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Leaky ReLU ফাংশনের সূত্র:\n",
    "\n",
    "\\[\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{যদি } x > 0 \\\\\n",
    "\\alpha x & \\text{যদি } x \\leq 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "যেখানে \\(\\alpha\\) একটি ছোট ধনাত্মক কনস্ট্যান্ট (যেমনঃ 0.01)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Leaky ReLU-এর বৈশিষ্ট্য\n",
    "\n",
    "- **ডাইং রিলু সমস্যা কাটায়:** নেগেটিভ ইনপুটেও গ্র্যাডিয়েন্ট থাকে\n",
    "- **নেগেটিভ ইনপুট বাদ না দিয়ে ছোট করে নেয়**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Parametric ReLU (PReLU) এর সূত্র:\n",
    "\n",
    "\\[\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{যদি } x > 0 \\\\\n",
    "a x & \\text{যদি } x \\leq 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "- এখানে \\(\\alpha\\) (বা \\(a\\)) একটি ট্রেইনেবল প্যারামিটার, যেটা মডেল নিজে শেখে\n",
    "- Leaky ReLU-তে \\(\\alpha\\) ফিক্সড থাকে, কিন্তু PReLU-তে এটি লার্ন হয়\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python কোডে Leaky ReLU এবং PReLU\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leaky ReLU ফাংশন\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# PReLU ফাংশন (alpha ট্রেইনেবল ধরেছি না, শুধু illustration)\n",
    "def prelu(x, alpha=0.2):  # assume alpha is learned\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# x value\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# plotting\n",
    "plt.plot(x, leaky_relu(x), label=\"Leaky ReLU (α=0.01)\")\n",
    "plt.plot(x, prelu(x), label=\"PReLU (α=0.2)\")\n",
    "plt.title(\"Leaky ReLU ও PReLU ফাংশন\")\n",
    "plt.xlabel(\"Input (x)\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. তুলনা: Leaky ReLU vs ReLU vs PReLU\n",
    "\n",
    "| বৈশিষ্ট্য        | ReLU        | Leaky ReLU         | PReLU               |\n",
    "|------------------|-------------|---------------------|----------------------|\n",
    "| নেগেটিভ ইনপুট   | 0           | αx (α ছোট)         | a*x (a শিখে নেয়)     |\n",
    "| α ট্রেইনেবল?     | না          | না                  | হ্যাঁ                |\n",
    "| ডাইং রিলু সমস্যা | থাকে        | কম থাকে            | আরও কম থাকে          |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. কখন ব্যবহার করবেন?\n",
    "\n",
    "- **Leaky ReLU:** যদি ReLU ব্যবহারে ডাইং নিউরনের সমস্যা দেখা যায়\n",
    "- **PReLU:** যখন আপনি চান মডেল নিজেই নেগেটিভ স্লোপ শিখুক, বেশি ফ্লেক্সিবিলিটি দরকার\n",
    "\n",
    "---\n",
    "\n",
    "## 7. সংক্ষিপ্তসার:\n",
    "\n",
    "- Leaky ReLU এবং PReLU হলো ReLU এর উন্নত সংস্করণ\n",
    "- এরা নেগেটিভ ইনপুটের জন্য সামান্য আউটপুট দেয়, যাতে গ্র্যাডিয়েন্ট 0 না হয়\n",
    "- PReLU ট্রেইনেবল এবং বেশি ফ্লেক্সিবল, কিন্তু কমপ্লেক্স ও বেশি রিসোর্স নেয়\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e6a52-cffa-4783-a752-7a09a72acb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
