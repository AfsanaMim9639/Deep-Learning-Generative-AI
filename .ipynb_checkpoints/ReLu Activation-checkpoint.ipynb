{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c1d070-fc20-4939-b24d-7286ca42eafd",
   "metadata": {},
   "source": [
    "# ReLU অ্যাক্টিভেশন ফাংশন\n",
    "\n",
    "**ReLU** (Rectified Linear Unit) হলো বর্তমানে নিউরাল নেটওয়ার্কে সবচেয়ে বেশি ব্যবহৃত অ্যাক্টিভেশন ফাংশন। এটি ইনপুটের উপর ভিত্তি করে সহজ একটি শর্ত ভিত্তিক ফাংশন, যেখানে নেগেটিভ ইনপুটকে 0 করা হয় এবং পজিটিভ ইনপুট অপরিবর্তিত থাকে।\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ReLU ফাংশনের সূত্র:\n",
    "\n",
    "\\[\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\]\n",
    "\n",
    "যেখানে:\n",
    "- যদি \\(x > 0\\), তাহলে \\(\\text{ReLU}(x) = x\\)\n",
    "- যদি \\(x \\leq 0\\), তাহলে \\(\\text{ReLU}(x) = 0\\)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ReLU ফাংশনের বৈশিষ্ট্য:\n",
    "\n",
    "- **সহজ এবং দ্রুত গণনা করা যায়**\n",
    "- **ভ্যানিশিং গ্র্যাডিয়েন্ট সমস্যা অনেক কম হয়**\n",
    "- **স্পারস অ্যাক্টিভেশন** (অনেকগুলো নিউরন 0 হয়, যা মেমোরি সাশ্রয় করে)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ReLU ফাংশনের আচরণ:\n",
    "\n",
    "### ক. যখন x > 0:\n",
    "- আউটপুট হবে \\(x\\)\n",
    "\n",
    "### খ. যখন x ≤ 0:\n",
    "- আউটপুট হবে 0\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ReLU ফাংশনের ডেরিভেটিভ:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx} \\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{যদি } x > 0 \\\\\n",
    "0 & \\text{যদি } x \\leq 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ReLU ফাংশনের সুবিধা:\n",
    "\n",
    "- **সহজ গণনা:** কম্পিউটেশনের জন্য অনেক দ্রুত\n",
    "- **ভ্যানিশিং গ্র্যাডিয়েন্ট সমস্যা নেই:** পজিটিভ রেঞ্জে গ্র্যাডিয়েন্ট সবসময় 1 থাকে\n",
    "- **স্পারস অ্যাক্টিভেশন:** নেগেটিভ ইনপুটে 0 হয়, যা মডেলকে আরও কার্যকর করে তোলে\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ReLU ফাংশনের অসুবিধা:\n",
    "\n",
    "- **ডাইং রিলু (Dying ReLU) সমস্যা:** যদি অনেক নিউরনের ইনপুট সবসময় 0 বা নেগেটিভ হয়, তবে সেই নিউরনগুলো ট্রেইনিংয়ে অংশ নেয় না (গ্র্যাডিয়েন্ট 0 হয়)\n",
    "- **নেগেটিভ ইনপুটের জন্য ডেরিভেটিভ 0:** ফলে কোনো আপডেট হয় না\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Python কোড দিয়ে ReLU ফাংশনের উদাহরণ:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ReLU ফাংশন সংজ্ঞায়িত করা\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# x এর একটি রেঞ্জ তৈরি করা\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# ReLU ফাংশন প্রয়োগ করা\n",
    "y = relu(x)\n",
    "\n",
    "# গ্রাফ প্লট করা\n",
    "plt.plot(x, y)\n",
    "plt.title(\"ReLU Function\")\n",
    "plt.xlabel(\"Input (x)\")\n",
    "plt.ylabel(\"Output (ReLU(x))\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. ReLU বনাম Sigmoid বনাম Tanh তুলনা\n",
    "\n",
    "| বৈশিষ্ট্য     | Sigmoid      | Tanh         | ReLU           |\n",
    "|---------------|--------------|--------------|----------------|\n",
    "| রেঞ্জ         | 0 থেকে 1     | –1 থেকে +1   | 0 থেকে ∞       |\n",
    "| সেন্টারড      | না           | হ্যাঁ        | না             |\n",
    "| গ্র্যাডিয়েন্ট | দুর্বল       | মাঝারি       | শক্তিশালী (x>0) |\n",
    "| কম্পিউটেশন   | ধীর          | ধীর          | দ্রুত          |\n",
    "| সমস্যা        | ভ্যানিশিং গ্র্যাডিয়েন্ট | ভ্যানিশিং গ্র্যাডিয়েন্ট | ডাইং রিলু        |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. ReLU কখন ব্যবহার করবেন?\n",
    "\n",
    "- যখন বড় এবং গভীর নিউরাল নেটওয়ার্ক তৈরি করছেন\n",
    "- স্পিড এবং পারফরম্যান্স গুরুত্বপূর্ণ\n",
    "- সাধারণত ডিফল্ট অ্যাক্টিভেশন ফাংশন হিসেবে ব্যবহৃত হয়\n",
    "\n",
    "---\n",
    "\n",
    "## 10. সংক্ষিপ্তসার:\n",
    "\n",
    "- ReLU ফাংশন হলো নিউরাল নেটওয়ার্কে আজকাল সবচেয়ে জনপ্রিয় অ্যাক্টিভেশন ফাংশন\n",
    "- এটি কম্পিউটেশনের জন্য দ্রুত এবং ট্রেইনিং প্রসেসকে সহজ করে\n",
    "- তবে ডাইং রিলু সমস্যা বিবেচনায় রাখতে হয়\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebabea-f113-48b6-b85a-41ab8f8c8768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
