{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9debdac-c542-43e8-8108-cdaefa112bee",
   "metadata": {},
   "source": [
    "<h2>Adagrad Optimizer</h2>\n",
    "\n",
    "<h3>What is Adagrad?</h3>\n",
    "<ul>\n",
    "  <li><b>English:</b> Adagrad (Adaptive Gradient Algorithm) is an optimizer that adapts the learning rate for each parameter individually. It gives smaller learning rates to parameters associated with frequently occurring features, and larger learning rates to infrequent ones.</li>\n",
    "  <li><b>বাংলা:</b> Adagrad একটি অপটিমাইজার যা প্রতিটি প্যারামিটারের জন্য স্বয়ংক্রিয়ভাবে লার্নিং রেট অ্যাডজাস্ট করে। এটি যেসব ফিচার বেশি ঘন ঘন আসে, তাদের জন্য ছোট লার্নিং রেট এবং যেসব কম আসে, তাদের জন্য বড় লার্নিং রেট ব্যবহার করে।</li>\n",
    "</ul>\n",
    "\n",
    "<h3>How Does It Work?</h3>\n",
    "<ul>\n",
    "  <li><b>English:</b> Adagrad maintains a sum of the squares of all past gradients for each parameter. When updating weights, it divides the learning rate by the square root of this sum. This slows down the learning for frequently updated parameters.</li>\n",
    "  <li><b>বাংলা:</b> Adagrad প্রতিটি প্যারামিটারের জন্য পূর্ববর্তী গ্র্যাডিয়েন্টগুলোর বর্গের যোগফল রাখে। ওজন আপডেট করার সময় এটি সেই যোগফলের বর্গমূল দিয়ে লার্নিং রেট ভাগ করে দেয়। এর ফলে যেসব প্যারামিটার বেশি আপডেট হয়, তাদের জন্য লার্নিং রেট ধীরে ধীরে কমে যায়।</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Mathematical Formula</h3>\n",
    "<pre>\n",
    "θ = θ - (η / sqrt(G + ε)) * ∇J(θ)\n",
    "\n",
    "Where:\n",
    "- θ = parameter (weight)\n",
    "- η = learning rate\n",
    "- G = sum of squares of past gradients\n",
    "- ε = small constant to avoid division by zero\n",
    "- ∇J(θ) = gradient of the loss function\n",
    "</pre>\n",
    "\n",
    "<h3>Advantages of Adagrad</h3>\n",
    "<ul>\n",
    "  <li><b>English:</b> No need to manually tune learning rate.</li>\n",
    "  <li>Good for sparse data (e.g., text data or NLP tasks).</li>\n",
    "  <li><b>বাংলা:</b> লার্নিং রেট ম্যানুয়ালি টিউন করার দরকার নেই।</li>\n",
    "  <li>স্পার্স ডেটার জন্য (যেমন NLP) ভালো কাজ করে।</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Disadvantages of Adagrad</h3>\n",
    "<ul>\n",
    "  <li><b>English:</b> Accumulated gradient squares keep growing, which can make the effective learning rate shrink too much and cause the training to stop prematurely.</li>\n",
    "  <li><b>বাংলা:</b> আগের গ্র্যাডিয়েন্টের বর্গের যোগফল বাড়তে থাকে, যার ফলে লার্নিং রেট অনেক কমে গিয়ে ট্রেনিং থেমে যেতে পারে।</li>\n",
    "</ul>\n",
    "\n",
    "<h3>When to Use Adagrad?</h3>\n",
    "<ul>\n",
    "  <li><b>English:</b> Adagrad is useful when working with sparse data or features, such as in natural language processing (NLP), recommendation systems, or computer vision with rare events.</li>\n",
    "  <li><b>বাংলা:</b> যখন স্পার্স ডেটা বা বিরল ফিচার নিয়ে কাজ করা হয়, যেমন NLP, রিকমেন্ডেশন সিস্টেম ইত্যাদিতে Adagrad ব্যবহার করা ভালো।</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Keras Example</h3>\n",
    "<pre>\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "optimizer = Adagrad(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "</pre>\n",
    "\n",
    "<h3>Summary Table</h3>\n",
    "<table border=\"1\" cellpadding=\"5\">\n",
    "  <tr>\n",
    "    <th>Aspect</th>\n",
    "    <th>Adagrad</th>\n",
    "    <th>বাংলা</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Learning Rate</td>\n",
    "    <td>Automatically adjusted per parameter</td>\n",
    "    <td>প্রতি প্যারামিটারে আলাদা লার্নিং রেট</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Strength</td>\n",
    "    <td>Great for sparse data</td>\n",
    "    <td>স্পার্স ডেটার জন্য চমৎকার</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Limitation</td>\n",
    "    <td>Learning rate may shrink too much</td>\n",
    "    <td>লার্নিং রেট অনেক কমে যেতে পারে</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a78b17-69f8-4e5a-b69d-a2d07c2ee7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
